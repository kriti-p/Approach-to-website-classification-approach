{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement :\n",
    "\n",
    "There are 10,000 websites of Car service providers. Now, what approaches will you follow to\n",
    "make a classifier model in ML to make the categorization of those websites under some\n",
    "keywords? Explain your logic and make a list of requirements including python libraries,\n",
    "datasets, etc that will be needed to successfully run the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website classification\n",
    "Known as a web page categorization.  It is a type of supervised learning problem that aims to categorize web pages into a set of predefined categories based on labeled training data.\n",
    "\n",
    "A classification problem occurs when an object needs to be assigned into a predefined group or class based on a number of observed attributes related to that object. It is the task of determining whether a web page belongs to a category or categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model\n",
    "\n",
    "For any machine learning algorithm, we need some training set and test set for training the model and testing the accuracy of that model. For classifier model in ML to make the categorization of those websites under some keywords. We create the set of data for the model, we already have the keywords from different websites, we will just classify them according to the class, and then apply the results in the next module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Classification techniques\n",
    "\n",
    "There are several classificatio technique such as artificial neural network, decision tree, deep learning, knn, logistic regression, naïve bayes, SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of requirements :\n",
    "\n",
    "1. Module for web data scraping : requests\n",
    "\n",
    "2. Module to extract complete text data from the HTML page : BeautifulSoup \n",
    "\n",
    "3. Module to find keywords inside the text recieved from the the websites : KeywordProcessor from flashtext.keyword \n",
    "\n",
    "4. Module to to import our dataset : pandas\n",
    "\n",
    "5. Module to exploration and analysis : numpy\n",
    "\n",
    "6. Module to create a bag of words : NLTK Lancaster Stemmer\n",
    "\n",
    "7. Module to train the model : time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods/Functions\n",
    "\n",
    "1. requests.get() : request the page from website using https protocol and load the page into the object “page”.\n",
    "    \n",
    "2. findall() : finds all visible text from code and return a list of Strings which we store in texts.\n",
    "\n",
    "3. join() : join all individual text into a common string.\n",
    "    \n",
    "4. extract_keywords(string) : to find keywords present in the text.\n",
    "\n",
    "5. Sigmoid Function\n",
    "\n",
    "6. Cleaning function\n",
    "\n",
    "7. Bag Of Words function\n",
    "\n",
    "8. Think function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "It consists of crawling web site and getting that particular html document from site and then cleansing the document and storing it into database.\n",
    "\n",
    "We can use open data from sites like Kaggle or Data.gov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Approach :\n",
    "\n",
    "Here we have 10,000 website of car service provider so for these website we can opt flat and hierarchical classification where categories are parallel in the former and organized in a hierarchical tree structure in the latter, in which each category may have several subcategories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " So we can classify websites into following categories:\n",
    "\n",
    "In each of the category there are several subcategeroies belonging to particular category, we will match those keywords with the text and find the class with the maximum Matching_value.\n",
    "\n",
    "\n",
    "The approach here is that we will have certain keywords belonging to the particular category, and \n",
    "\n",
    "Matching_value = (Number of keywords matched with one industry)/(Total number of keywords matched)\n",
    "\n",
    "So accordingly we have a list of keywords for the individual categories as follows:\n",
    "\n",
    "Class_1_keywords = ['Hundai', Service','Washing','repair','Exchange', 'kia', 'accesseries'] \n",
    "\n",
    "Class_2_keywords = [ 'Maruti', 'Toyoto','Phone', 'glass', 'seatcover', 'headlight']\n",
    "\n",
    "Class_3_keywords = ['tire','steering', 'battery', 'oil', 'AC', 'honda','autoparts'] \n",
    "\n",
    "\n",
    "We can add many more keywords in any or all of the classes to get more accurate result.\n",
    "\n",
    "\n",
    "keyword = Class_1_keywords + Class_2_keywords + Class_3_keywords\n",
    "\n",
    "Now, we will use KeywordProcessor to find keywords inside the text recieved from the the urls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the years SVM and neural network classifiers are mostly used. Although we can use any of the above mentioned ML algirithom to build the model.  It has also been noted that deep learning is increasingly being chosen in the last three years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "ANN is also called a \"Neural Network\" (NN). It is a mathematical and computational model based on biological neural networks. It consists of an interconnected group of artificial neurons that  processes information using an approach to computation. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The car websites classification method uses a neural network with inputs obtained by both the principal components analysis (PCA) and class profile based features (CPBF). Each news web page is represented by the term-weighting scheme. \n",
    "\n",
    "The PCA has been used to select the most relevant features for the classification. Then the final output of the PCA is combined with the feature vectors from the class-profile, which contains the most regular keywords in each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training phase, the correct class for each keyword is known as supervised training and the output nodes can be assigned \"1\" for the node corresponding to the correct class, and \"0\" for the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
